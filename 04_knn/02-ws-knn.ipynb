{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsKIgxnRTr9d"
   },
   "source": [
    "# Классификация\n",
    "\n",
    "### Литература\n",
    "\n",
    "- [Ближайшие соседи - sklearn](http://scikit-learn.org/stable/modules/neighbors.html)\n",
    "- [ODS классификация](https://habrahabr.ru/company/ods/blog/322534/#metod-blizhayshih-sosedey)\n",
    "- [Семинары по машинному обучению, ВМК МГУ. kNN-1](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem02_knn.pdf)\n",
    "- [Семинары по машинному обучению, ВМК МГУ. kNN-2](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem03_knn.pdf)\n",
    "- [wiki: Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "## K ближайших соседей kNN\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png)\n",
    "\n",
    "kNN - это метрический алгоритм, предполагающий, что объекты одного класса в пространстве находятся близко друг к другу. В зависимости от выбора k - количества ближайших соседей и метрики расстояния, качество может сильно отличаться. Страдает от проклятия размерностей. Ленивый, значит проивзодит все вычисления при предсказании, а не при обучении. Хранит всю тренировочную выборку в себе.\n",
    "\n",
    "Идея алгоритма:\n",
    "1. Взять новый объект и вычислить все расстояния по некоторой метрике от него до других объектов\n",
    "2. Выбрать k ближайших соседей к этому объекту\n",
    "3. Класс объекта - это класс наиболее часто встречающегося объекта среди k соседей.\n",
    "\n",
    "В алгоритм можно внести изменение добавив веса для каждого объекта или класса. Например, при выборе класса смотрят не на большинство соседей, а на какую-то взвешенную сумму.\n",
    "\n",
    "Кстати алгоритм можо свести к регрессии, если брать среднее значение целевой переменной по k соседям.\n",
    "\n",
    "### Метод ближайших соседей в реальных задачах\n",
    "\n",
    "- В чистом виде kNN может послужить хорошим стартом (baseline) в решении какой-либо задачи;\n",
    "- В соревнованиях Kaggle kNN часто используется для построения мета-признаков (прогноз kNN подается на вход прочим моделям) или в стекинге/блендинге;\n",
    "- Идея ближайшего соседа расширяется и на другие задачи, например, в рекомендательных системах простым начальным решением может быть рекомендация какого-то товара (или услуги), популярного среди ближайших соседей человека, которому хотим сделать рекомендацию;\n",
    "- На практике для больших выборок часто пользуются приближенными методами поиска ближайших соседей. Вот лекция Артема Бабенко про эффективные алгоритмы поиска ближайших соседей среди миллиардов объектов в пространствах высокой размерности (поиск по картинкам). Также известны открытые библиотеки, в которых реализованы такие алгоритмы, спасибо компании Spotify за ее библиотеку Annoy.\n",
    "\n",
    "### Качество классификации методом ближайших соседей зависит от нескольких параметров:\n",
    "\n",
    "- число соседей\n",
    "- метрика расстояния между объектами (часто используются метрика Хэмминга, евклидово расстояние, косинусное расстояние и расстояние Минковского). Отметим, что при использовании большинства метрик значения признаков надо масштабировать. Условно говоря, чтобы признак \"Зарплата\" с диапазоном значений до 100 тысяч не вносил больший вклад в расстояние, чем \"Возраст\" со значениями до 100.\n",
    "- веса соседей (соседи тестового примера могут входить с разными весами, например, чем дальше пример, тем с меньшим коэффициентом учитывается его \"голос\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EGn83YyTr9h"
   },
   "source": [
    "### sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "[Документация](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
    "\n",
    "Основные параметры класса sklearn.neighbors.KNeighborsClassifier:\n",
    "\n",
    "- weights: \"uniform\" (все веса равны), \"distance\" (вес обратно пропорционален расстоянию до тестового примера) или другая определенная пользователем функция\n",
    "- algorithm (опционально): \"brute\", \"ball_tree\", \"KD_tree\", или \"auto\". В первом случае ближайшие соседи для каждого тестового примера считаются перебором обучающей выборки. Во втором и третьем — расстояние между примерами хранятся в дереве, что ускоряет нахождение ближайших соседей. В случае указания параметра \"auto\" подходящий способ нахождения соседей будет выбран автоматически на основе обучающей выборки.\n",
    "- leaf_size (опционально): порог переключения на полный перебор в случае выбора BallTree или KDTree для нахождения соседей\n",
    "- metric: \"minkowski\", \"manhattan\", \"euclidean\", \"chebyshev\" и другие\n",
    "\n",
    "_взято из курса [ODS](https://habrahabr.ru/company/ods/blog/322534/#metod-blizhayshih-sosedey)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhSvLCIGTr9i"
   },
   "source": [
    "### Метрики расстояния\n",
    "\n",
    "Аксиомы метрики:\n",
    "\n",
    "1. $\\rho(x,y) \\ge 0$\n",
    "2. $\\rho(x,y) = \\rho(y,x)$\n",
    "3. $\\rho(x,y) \\ge \\rho(x,z) + \\rho(z,y)$\n",
    "\n",
    "#### Евклидова метрика\n",
    "\n",
    "$\\large \\rho(x,y) = \\sqrt{\\sum_{i=1}^{n}{(x_i - y_i)^2}}$\n",
    "\n",
    "#### Манхэттенская метрика\n",
    "\n",
    "$\\large \\rho(x,y) = \\sum_{i=1}^{n}{|x_i - y_i|}$\n",
    "\n",
    "####  Минковского метрика\n",
    "\n",
    "$\\large \\rho(x,y) = {(\\sum_{i=1}^{n}{|x_i - y_i| ^ q})} ^ \\frac{1}{q}$\n",
    "\n",
    "#### Косинусная метрика\n",
    "\n",
    "$\\large \\rho(x,y) = \\arccos(\\frac{\\langle{x,y}\\rangle}{\\|x\\|\\|y\\|}) = \\arccos{\\frac{\\sum^d_{i=1}{x_i y_i}}{(\\sum^d_{i=1}{x_i^2})^{1/2}(\\sum^d_{i=1}{y_i^2})^{1/2}}}$\n",
    "\n",
    "![](https://i.imgur.com/0aBH1bO.png)\n",
    "\n",
    "### Масштаб признаков \n",
    "\n",
    "Исходя из формулы метрики расстояния между объектами в пространстве, можно сделать вывод, что масштаб признаков определяет их значимость. Например, умножим один из признаков на константу C. Тогда Евклидово расстояние примет вид: $$p_2(x,y) = \\sqrt{C(x_1-y_1)^2+\\sum_{i=2}^{d}{(x_i-y_i)^2}}$$ Таким образом, различие по первому признаку будет считаться в C раз более значимым, чем различия по всем остальным признакам. При этом расположение объектов относительно друг друга не изменилось - изменился лишь масштаб!\n",
    "\n",
    "Если мы возьмем задачу, в которой измеряется рост в сантиметрах и вес в килограммах людей, и будем на этих данных предсказывать что-то, то рост будет всегда иметь большую важность, чем вес.  \n",
    "Чтобы избежать проблему масштаба, признаки нужно нормировать. Это можно делать следующими способами: \n",
    "\n",
    "- Нормировка на единичную дисперсию: $\\widetilde{x} = \\frac{x - \\overline{x}}{\\sigma(x)}$\n",
    "- Нормировка на отрезок [0,1]: $\\widetilde{x} = \\frac{x - min(x)}{max(x) - min(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cbR6xG8Tr9l"
   },
   "source": [
    "### Проклятье размерности\n",
    "\n",
    "Проблема заключается в невозможности эффективного поиска ближайших соседей для заданной точки в многомерном пространстве. Это происходит из-за того, что все объекты выборки равномерно распределены по d-мерной сфере и равноудалены друг от друга. Например, для kNN для 5 соседей и 5000 обхектов в выборке размерность должна быть не больше 10, чтобы решение было более менее эффективным. \n",
    "\n",
    "Подробнее об этом прочитайте в материалах к [Семинарам по машинному обучению, ВМК МГУ](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem02_knn.pdf).\n",
    "\n",
    "![](https://i.imgur.com/VZkE79X.png)\n",
    "\n",
    "$\\LARGE \\lim_{n \\rightarrow \\infty}{0.99^n} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pZFUdyy6w1r"
   },
   "source": [
    "Также будет полезно вспомнить, что такое скользящий контроль (кросс-валидация).\n",
    "\n",
    "![image.png](https://habrastorage.org/files/b1d/706/e6c/b1d706e6c9df49c297b6152878a2d03f.png)\n",
    "\n",
    "Кратко: выборка разбивается на N равных частей, и проводится N экспериментов, причём для i-го эксперимента i-ая часть выборки не используется в обучении, но используется в предсказании для оценки обобщающей способности модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QG4SiqrTr9o"
   },
   "source": [
    "## Практика\n",
    "\n",
    "### Классификация\n",
    "\n",
    "Рассмотрим наш любимый датасет с цифрами MNIST. Мы его уже решали весьма успешно с помощью одного метрического алгоритма кластеризации, поэтому возможно тут тоже будет всё хорошо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dr7BzwrnTr9s"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Зафиксируем случайность, чтобы каждый раз получалось одно и тоже\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "jRyHY2NxTr96",
    "outputId": "a6b07446-02f7-49e5-932a-2464140acab7"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# загружаем датасет с цифрами\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "print(\"Экземпляров: {}\\nРазмер изображения: {}x{}\".format(X.shape[0], np.sqrt(X.shape[1]), np.sqrt(X.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "extfJ78oTr-B",
    "outputId": "dac85ee5-dd6c-4db2-a4d8-d8713b3ef183"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "width = int(np.sqrt(X.shape[1]))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X[i,:].reshape([width,width]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7S25wfzTr-N"
   },
   "source": [
    "Как мы с вами помним, классификацию по очень несбалансированным выборкам делать сложно и нужно брать специальные метрики и хитрые разбиения на трейн-тест. Посмотрим, как у нас дела обстоят с цирфами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "GNcdoYL4Tr-P",
    "outputId": "83f5c81f-1583-4bd4-d508-75dd9115101f"
   },
   "outputs": [],
   "source": [
    "sns.countplot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4aIsUvWxNLe"
   },
   "source": [
    "Реализуем kNN самостоятельно. Нам будет достаточно написать функцию вычисления класса. (почему? а как же обучение модели?)\n",
    "\n",
    "Предлагаю Вам сделать так, чтобы метрику можно было менять. Проще всего сделать её аргументом функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1r3OUwdvx0ff"
   },
   "outputs": [],
   "source": [
    "def euclidian(x):  # норма в Евклидовом пространстве\n",
    "    pass\n",
    "\n",
    "def euclidian_metric(a, b):  # реализуем Евклидову метрику через норму\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTQCv5P06ej"
   },
   "source": [
    "Далее реализуем функции, необходимые нам для вычисления класса по методу ближайших соседей.\n",
    "\n",
    "1.Напишите функцию, которая по уже рассчитанным расстояниям от точек `y` до нашей точки (`distances`) находит и возвращает `k` ближайших соседей и соответствующие расстояния до них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPmRKGbclRw3"
   },
   "outputs": [],
   "source": [
    "def _find_neighbours(k, y, distances):\n",
    "    #############\n",
    "    #  ВАШ КОД  #\n",
    "    #############\n",
    "    return neighbours, neighbours_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "W60Xt4aNTCcS",
    "outputId": "40f09fc2-6ee7-447a-c070-cd9b1d0c3b3a"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "Y = np.arange(10)\n",
    "dist = np.linspace(1, 10, 10)\n",
    "_find_neighbours(3, Y, dist)  # должен вернуться кортеж из массивов [0, 1, 2] и [1, 2, 3], если вернулось что-то другое, то что-то не так реализовано"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvJZgVc1lUX7"
   },
   "source": [
    "2.Реализуйте функцию, выделяющую преобладающий класс среди соседних объектов. Если есть конкурирующие классы, то нужно вернуть их все.\n",
    "\n",
    "Конкурирующими считаются те классы, для которых количество принадлежащих им объектов равно между собой и при этом наибольшее среди всех классов, представленных в подвыборке.\n",
    "\n",
    "**Hint:** используйте `np.unique`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHksfxfCc0dy"
   },
   "outputs": [],
   "source": [
    "def _get_closest_classes(neighbours):\n",
    "    #############\n",
    "    #  ВАШ КОД  #\n",
    "    #############\n",
    "    return best_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "CX0B5kNJRJ6P",
    "outputId": "9805eadd-6bf5-4255-ad78-493152086af4"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(_get_closest_classes(np.asarray([1,2,3,2,2])))  # должна вернуться только \"2\"\n",
    "print(_get_closest_classes(np.asarray([1,2,3,2,3])))  # должны вернуться \"2\" и \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJI0Z5Itlfc4"
   },
   "source": [
    "3.Напишите функцию, которая выбирает наиболее подходящий класс, при условии, что есть несколько конкурирующих классов. Для этого будет необходимо посчитать среднее расстояние до объектов каждого класса и выбрать класс с наименьшим таким расстоянием."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv2HgEF7ld95"
   },
   "outputs": [],
   "source": [
    "def _choose_best_class(best_classes, neighbours, neighbouring_distances):\n",
    "    min_mean_dist = np.inf\n",
    "    best_class = None\n",
    "\n",
    "    #############\n",
    "    #  ВАШ КОД  #\n",
    "    #############\n",
    "\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "D-Qp-MKWVUPS",
    "outputId": "e75d8c1a-47fd-4502-8d7d-1c7ecc22d6bd"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "_choose_best_class([1,2], np.array([1, 2, 1, 3, 2]), np.asarray([0.5, 1, 1, 8, 0.6]))  # если всё правильно, best_class должен быть \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD01ReK4VXC2"
   },
   "source": [
    "Далее нужно реализовать функцию, делающую предсказание для одной точки, используя реализованные ранее функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXogyqZS1d7E"
   },
   "outputs": [],
   "source": [
    "# эта функция будет считать расстояния от новой точки new_x до всех точек в исходном датасете X и на основе расстояний вычислять принадлежность к классу\n",
    "def _nearest_neighbours_classify(x, y, k, new_x, metric):\n",
    "    distances = metric(x, new_x)  # считаем расстояния до классов\n",
    "\n",
    "    neighbours, neighbouring_distances = _find_neighbours(k, y, distances)  # находим ровно k соседей этой точки\n",
    "        \n",
    "    best_classes = _get_closest_classes(neighbours)  # обнаруживаем классы, которые имеются среди соседей\n",
    "\n",
    "    res = _choose_best_class(best_classes, neighbours, neighbouring_distances)  # выбираем наиболее релевантный класс по среднему расстоянию до него среди соседей\n",
    "\n",
    "    return res\n",
    "        \n",
    "# а в этой функции мы повторяем это всё для каждого элемента нашей выборки. Короче говоря, обрабатываем сразу батч (на самом деле не сразу, а по одной точке, медленно, но понятно).\n",
    "def nearest_neighbours_classify(x, y, k, x_pred, metric=euclidian_metric):\n",
    "    res = np.zeros(x_pred.shape[0], dtype=y.dtype)\n",
    "    for i in range(x_pred.shape[0]):\n",
    "        res[i] = _nearest_neighbours_classify(x, y, k, x_pred[i], metric)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmZtWnFEpDN0"
   },
   "source": [
    "Запустим в лоб наш классификатор kNN и посмотрим, какое качество он нам выдаст. Воспользуемся train_test_split, чтобы разбить данные на тренировочную и валидационную выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rQF6rRS5pRKo",
    "outputId": "7d4c4f2e-21b5-457c-b01f-0e324b57541e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "y_pred = nearest_neighbours_classify(X_train, y_train, 5, X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgTUDMWoTr-X"
   },
   "source": [
    "Целевой признак распределен равномерно по всем классам с некоторой погрешностью. Поэтому можно считать в лоб обычную точность предсказания и не бояться, что интерпретация будет неадекватна. НО при кроссвалидации происходят случайные подвыборки объектов, а нам нужно особым образом стратифицировать разбиение, чтобы сохранялись пропорции классов. Для этих целей применяется стратификация.\n",
    "\n",
    "Подробнее о стратификации вы можете [почитать тут](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html#stratification). \n",
    "\n",
    "Мы не будем реализовывать стратификацию самостоятельно (т.к. это очень муторно), а воспользуемся готовой реализацией из sklearn. Всё остальное возмём оттуда же, т.к. наш kNN сейчас идентичен тому, что в sklearn.\n",
    "Для выделения стратифицированной выборки будем использовать [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaYqaj3_Tr-H"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # класс для kNN классификатора\n",
    "\n",
    "from sklearn.model_selection import cross_val_score # метод для кросс-валидации данных\n",
    "\n",
    "from sklearn.model_selection import KFold # алгоритм разбиения выборки на группы(фолды)\n",
    "from sklearn.model_selection import StratifiedKFold # алгоритм разбиения выборки на стратифицированные группы(фолды)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43nsFF2zTr-Y"
   },
   "source": [
    "Запустим в лоб классификатор kNN и посмотрим на качество на кроссвалидации при разбиении выборки на 5 фолдов со стратификацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "0_0sQoxuTr-Z",
    "outputId": "fdaa7f0f-c36d-44f5-cc03-c36c9bb2bd33"
   },
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "%time scores = cross_val_score(clf, X, y, cv=cv)\n",
    "\n",
    "print(\"Accuracy: {}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1ymswn-Tr-e"
   },
   "source": [
    "Отлично, хорошая точность в 0.98, но где у нас сосредоточены ошибки? В чём проблема предсказания? Хотелось бы получить красивый отчет, вдруг мы просто неправильно предсказываем часть 1 и 7??\n",
    "\n",
    "Воспользуемся функцией [classification-report](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-report), которая выведет precision, recall, f1-score, support и confusion_matrix, по которой мы поймем что с чем путает алгоритм.\n",
    "\n",
    "![](https://i.imgur.com/8xhLDz8.png)\n",
    "\n",
    "$\\LARGE precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "$\\LARGE recall = \\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "ft-SZmC3Tr-g",
    "outputId": "4c5be7bd-606d-4447-8297-96257fb80c28"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# разобъем датасет на train и test в пропорции 60/40\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.4)\n",
    "y_test = y_test.astype('int')\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train) # обучим модель\n",
    "%time y_pred = clf.predict(X_test).astype('int') # предскажем тэги на тестовой подвыборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "pYPfF5lDTr-m",
    "outputId": "6c0a9cc3-7151-4ca1-83a6-c688ce26d381"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred)) # напечатаем отчет о классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Py9T4FNOTr-w"
   },
   "source": [
    "Построем [confusion матрицу](https://en.wikipedia.org/wiki/Confusion_matrix), для того, чтобы посмотреть какие классы путает алгоритм. При идеальном предсказании матрица должна быть диагональной. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "9sNZtbQ1Tr-y",
    "outputId": "88afb145-6076-4893-dfa4-5c244d6d906c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12)) \n",
    "_ = sns.heatmap(confusion_matrix(y_test, y_pred), cmap=plt.cm.Blues, square=True, annot=True, fmt='.4g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21YfoKJITr-3"
   },
   "source": [
    "На матрице видим, что влгоритм предсказывает часто 8, для 1. 9 для 7. 3 для 8. Возможно написания этих цифр действительно похожи.  \n",
    "К сожалению мы обучались не на полном датасете, а на его части, причем с пониженной размерностью, поэтому результаты не столь живописны :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yfiU-arTr-4"
   },
   "source": [
    "### Тюнинг гиперпараметров\n",
    "\n",
    "Как улучшить качество предсказания?\n",
    "\n",
    "Самый просто способ для того чтобы подобрать лучший параметр - это перебрать их ВСЕ с помощью [GridSearch](http://scikit-learn.org/stable/modules/grid_search.html)\n",
    "\n",
    "![](https://pp.userapi.com/c639616/v639616016/4938d/-9s9ffsvAC0.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "powhQDLKTr-5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3-6_NidqTr--",
    "outputId": "36f20ac8-3a48-4766-c0d6-c7e5ae5a324a"
   },
   "outputs": [],
   "source": [
    "np.arange(1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzcEYJDUTr_D"
   },
   "source": [
    "Найдем значения `n_neighbors` и `p` при котором качество классификации на кроссвалидации максимально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "xxuRXeZgTr_D",
    "outputId": "a098dc2f-d562-44a2-e356-c8c90fd19fde"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_neighbors\": np.arange(2, 10), \n",
    "    \"p\": [2,4]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(KNeighborsClassifier(), params, n_jobs=2, \n",
    "                      cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), verbose=2)\n",
    "%time search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "fuRpFJukTr_S",
    "outputId": "a3889ef2-9eab-4b60-9298-d2bfc18700ca"
   },
   "outputs": [],
   "source": [
    "x_ticks = [\"{:02d}_n_neighbors={}, p={}\".format(i, p['n_neighbors'], p['p']) for i,p in enumerate(search.cv_results_['params'])]\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(x_ticks, search.cv_results_['mean_test_score'])\n",
    "_ =plt.xticks(rotation=90)\n",
    "\n",
    "print(\"BEST: score={}, params={}\".format(search.best_score_, search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl84_mV0Tr_W"
   },
   "source": [
    "Посмотрим, стал ли лучше отчет классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 984
    },
    "id": "GSe8Y6K4Tr_Z",
    "outputId": "0b8bf81d-a9b0-452e-a097-c66829419f6c"
   },
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3, p=2)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test).astype('int')\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "_ = sns.heatmap(confusion_matrix(y_test, y_pred), cmap=plt.cm.Blues, square=True, annot=True, fmt='.3g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "cbTfOEA8Tr_d",
    "outputId": "97c7c6ed-fd6a-423c-fc21-c910ecc41087"
   },
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3, p=2)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "%time scores = cross_val_score(clf, X, y, cv=cv)\n",
    "print(\"Accuracy: {}\".format(scores.mean()))\n",
    "# с настройками по умолчанию было 0.9838396055603897"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDq_VnVqTr_g"
   },
   "source": [
    "## Подбор гиперпараметров\n",
    "\n",
    "Посмотрим на более наглядный пример подбора гипер параметров и важность масштабирования признаков.\n",
    "\n",
    "Рассмотрим датасет с винишком: https://www.openml.org/d/187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xZrHzst7bkY2",
    "outputId": "8b6a826c-5be7-4837-aff3-44c625a2caa4"
   },
   "outputs": [],
   "source": [
    "colab = True\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "I2qSh8tbTr_i",
    "outputId": "54e83e67-b17b-4d70-8dd5-a193224ccfae"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    df = pd.read_csv('/content/drive/My Drive/Data/dataset_191_wine.csv')\n",
    "else:\n",
    "    df = pd.read_csv('../../data/dataset_191_wine.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6CAuioqTr_l"
   },
   "outputs": [],
   "source": [
    "X = df.drop(['class'], axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "528mTqR5Tr_p"
   },
   "source": [
    "Переберем в лоб количество соседей `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "EOMqBMPUTr_q",
    "outputId": "8adaf475-fa26-4347-f6ed-4759b850217c"
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=False) # фиксируем разбиения! Выключаем перемешивание для повтора результатов\n",
    "\n",
    "k_vals = np.arange(1, 100, 1)\n",
    "quality_by_k = [\n",
    "    cross_val_score(KNeighborsClassifier(n_neighbors=k), X, y, cv=cv).mean()\n",
    "    for k in k_vals\n",
    "]\n",
    "\n",
    "print(\"Best K = {}\".format(k_vals[np.argmax(quality_by_k)]))\n",
    "plt.plot(k_vals, quality_by_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B4MGSfKTr_u"
   },
   "source": [
    "Наилучший результат при 1-м соседе? Выглядит как некоторая форма вырождения модели. \n",
    "\n",
    "Обратим внимание, что масштаб признаков очень разный. Необходимо нормировать признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "XI1WtiLbTr_v",
    "outputId": "604e968f-3ba3-41d2-f77e-e03d9e9e3702"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "NtJUlZ6aTr_z",
    "outputId": "bca8cd8e-1cf6-4499-fb6c-8ae216529518"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "X_scaled = scale(X) # включим масштабирование\n",
    "cv = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "k_vals = np.arange(1, 100, 1)\n",
    "quality_by_k = [\n",
    "    cross_val_score(KNeighborsClassifier(n_neighbors=k), X_scaled, y, cv=cv).mean()\n",
    "    for k in k_vals\n",
    "]\n",
    "\n",
    "print(\"Best K = {}\".format(k_vals[np.argmax(quality_by_k)]))\n",
    "plt.plot(k_vals, quality_by_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DHtOmRWTr_2"
   },
   "source": [
    "16 соседей - уже лучше. Да и качество стало не 0.65, а около 0.9 по accuracy.\n",
    "\n",
    "Теперь подберем наилучшую метрику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "M3lkWBBBTr_4",
    "outputId": "e2fbb23c-3dd3-46ca-b450-926ceaaac3ad"
   },
   "outputs": [],
   "source": [
    "X_scaled = scale(X)\n",
    "cv = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "p_vals = np.linspace(1, 10, 100)\n",
    "quality_by_p = [\n",
    "    cross_val_score(KNeighborsClassifier(n_neighbors=16, metric='minkowski', p=p), X_scaled, y, cv=cv).mean()\n",
    "    for p in p_vals\n",
    "]\n",
    "\n",
    "print(p_vals[np.argmax(quality_by_p)])\n",
    "plt.plot(p_vals, quality_by_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2ON3tsTTr_7"
   },
   "source": [
    "Победила манхэттенская метрика."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 06-theory-classification-kNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
